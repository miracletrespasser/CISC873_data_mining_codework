{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "TvYYKekK0GnE"
      },
      "outputs": [],
      "source": [
        "#Library import\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem Formulation: The goal is to predict the product ratings given the other features known for a product on Wish.com. The input is features of the products(prices,buyer, units sold etc). The output is a rating score of the product ranging from 1 to 5. A classfication & prediction data-mining function is required. One challenge is that the data might be noisy and incomplete, therefore some data cleansing method should be applied to fiil in the blanks and pruning the unwanted features. Another challenge is to select model paramters, so the model has to be tested several times for different paramter settings.Additionally, some not important features could be reduced, but some method needed to be applied to find them. An ideal solution would mean to construct a model that selection and has 0 train and test error after training, which means that it outputs the exactly rating for the product for its inputs."
      ],
      "metadata": {
        "id": "Eq-_atH34iV9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "QaBub4u-0GnG"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('train_new.csv').sample(frac=1) #shuffle\n",
        "#find ratings in integer labels\n",
        "data = data.loc[data['rating'].isin([1, 2, 3, 4, 5])]\n",
        "data = data.fillna(0)#fill missing value\n",
        "#cut features that shouldn't count towards the result\n",
        "data = data.drop(['merchant_id', 'merchant_profile_picture', 'id', 'tags'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "yhT_WGVB0GnG"
      },
      "outputs": [],
      "source": [
        "#train-validation data split\n",
        "msk = np.random.rand(len(data)) < 0.7\n",
        "tr = data[msk]\n",
        "val = data[~msk]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "XGG26Ign0GnH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7439b1bc-a26b-4e7d-f143-8aeef48809db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3641: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[k1] = value[k2]\n"
          ]
        }
      ],
      "source": [
        "#categorical data encoding\n",
        "dict_cat = {}\n",
        "\n",
        "\n",
        "# columns that are of categorical value\n",
        "cat_cols = tr.columns[tr.dtypes==object].to_list()\n",
        "\n",
        "\n",
        "\n",
        "def cat_digit(col):  \n",
        "    # build the mapping\n",
        "    encoded = col.astype('category').cat.codes\n",
        "    # store the mapping\n",
        "    dict_cat[col.name] = dict(zip(np.asarray(col), np.asarray(encoded)))\n",
        "    return encoded\n",
        "\n",
        "# for each categorical feature, apply cat_digit where we build the mapping and transform the data\n",
        "# this is for the training set (where we build the mapping)\n",
        "tr[cat_cols] = tr[cat_cols].apply(lambda col: cat_digit(col))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "yFX93Gl90GnI"
      },
      "outputs": [],
      "source": [
        "# missing value handling \n",
        "# then we will use the mappings built from the training set, to transform the validation set\n",
        "val[cat_cols] = val[cat_cols].apply(lambda col: col.map(dict_cat[col.name]))\n",
        "# for string values that not seen in training set, we replace it with -1\n",
        "val = val.fillna(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train-test data is already splitted in the dataset provided"
      ],
      "metadata": {
        "id": "qtb0GOSUSgST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.read_csv('test_new.csv').sample(frac=1) \n",
        "_id = test_data['id']\n",
        "test_data = test_data.fillna(0)\n",
        "#preprocessing feature selection\n",
        "test_data = test_data.drop(['merchant_id', 'merchant_profile_picture', 'id', 'tags'], axis=1)\n",
        "\n",
        "test_data[cat_cols] = test_data[cat_cols].apply(lambda col: col.map(dict_cat[col.name]))\n",
        "# again, not-seen string value filled with -1\n",
        "test_data = test_data.fillna(-1)"
      ],
      "metadata": {
        "id": "e0gbfTcaGZce"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the experimental protocol used and how was it carried out? What preprocessing steps are used? <br>\n",
        "The purporse of the experiment is to predict the product ratings given the other features known for a product on Wish.com.<br>\n",
        "The materials are pre-splitted datasets train_new.csv and test_new.csv. The evironment is google colab notebook. <br>\n",
        "Methodology:<br>\n",
        "Decision Tree,SVM and Naive bayes Model will be used to analyze and evaluate the data.<br>\n",
        "First, do some preprocessing with the original data(done in the walkthrough notebook).<br>\n",
        "Then, use the default setting without any parameter tuning or extra data preprocessing in the walkthrough notebook to set up the model. Train the model with untuned training data and generate validation score using validation data.<br>\n",
        "After that, tune the hyperparameter or do extra data preprocessing and repeat the process and see if there is improvements in validation score. At least 5 tunes should be done.<br>\n",
        "<br>\n",
        "The data preprocessing steps used here is missing value handling, categorical data encode and dataset split. For Decision tree and Naive bayes, no feature scaling method is used. For SVM the features are scaled.<br>\n"
      ],
      "metadata": {
        "id": "L4TAh5LyS5gf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Decision tree"
      ],
      "metadata": {
        "id": "3RE2hsJEB9NU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attempt1: default configuration"
      ],
      "metadata": {
        "id": "iadz6z4QCA6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Decision Tree\n",
        "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
        "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation"
      ],
      "metadata": {
        "id": "TUD0EEVFwm4_"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "_rcPwIZQ0GnI"
      },
      "outputs": [],
      "source": [
        "tr_y = tr['rating']\n",
        "#training data target values\n",
        "tr_x = tr.drop('rating', axis=1)\n",
        "#training data\n",
        "# Create Decision Tree classifer object\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "# Train Decision Tree Classifer\n",
        "clf = clf.fit(tr_x,tr_y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "A2KpjiJA0GnJ"
      },
      "outputs": [],
      "source": [
        "#validation data split\n",
        "val_y = val['rating']\n",
        "val_x = val.drop('rating', axis=1)\n",
        "pred_val = clf.predict(val_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "8NwAWRBB0GnJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e494ff0-47d2-42af-8222-13bdf14e8025"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5255255255255256\n"
          ]
        }
      ],
      "source": [
        "#Prediction\n",
        "val_score = f1_score(val_y, pred_val, average='micro')\n",
        "print(val_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attempt1: Feature reduction for low importance features, changing quality measuring method"
      ],
      "metadata": {
        "id": "go_VlDauCUGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#get the feature importance form previous attempt\n",
        "feat_importance = clf.tree_.compute_feature_importances(normalize=False)\n",
        "print(\"feat importance = \" + str(feat_importance))"
      ],
      "metadata": {
        "id": "yMflOh8v0egL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6716a69e-968e-465c-835a-fe382335bb9b"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "feat importance = [0.05396051 0.02187632 0.         0.00746259 0.00584514 0.0964197\n",
            " 0.         0.00443714 0.03638079 0.         0.01997378 0.00131579\n",
            " 0.00131579 0.00423379 0.00131579 0.         0.0306669  0.\n",
            " 0.00307018 0.00811404 0.00470648 0.02472375 0.03312997 0.01482469\n",
            " 0.02186171 0.05771063 0.00027296 0.         0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#construct dataframe for the feature importance\n",
        "df=pd.DataFrame({'Feature_names':tr_x.columns,'Importances':feat_importance})\n",
        "df.sort_values(by='Importances',ascending=False)"
      ],
      "metadata": {
        "id": "HgXKd70w0l2z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 959
        },
        "outputId": "14ba7622-939a-4cb9-961b-db554a97031e"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   Feature_names  Importances\n",
              "5                   rating_count     0.096420\n",
              "25               merchant_rating     0.057711\n",
              "0                          price     0.053961\n",
              "8          badge_product_quality     0.036381\n",
              "22                 merchant_name     0.033130\n",
              "16          countries_shipped_to     0.030667\n",
              "21                merchant_title     0.024724\n",
              "1                   retail_price     0.021876\n",
              "24         merchant_rating_count     0.021862\n",
              "10                 product_color     0.019974\n",
              "23        merchant_info_subtitle     0.014825\n",
              "19                  urgency_text     0.008114\n",
              "3                     units_sold     0.007463\n",
              "4                 uses_ad_boosts     0.005845\n",
              "20                origin_country     0.004706\n",
              "7            badge_local_product     0.004437\n",
              "13          shipping_option_name     0.004234\n",
              "18            has_urgency_banner     0.003070\n",
              "14         shipping_option_price     0.001316\n",
              "12   product_variation_inventory     0.001316\n",
              "11     product_variation_size_id     0.001316\n",
              "26  merchant_has_profile_picture     0.000273\n",
              "17               inventory_total     0.000000\n",
              "15           shipping_is_express     0.000000\n",
              "9            badge_fast_shipping     0.000000\n",
              "6                   badges_count     0.000000\n",
              "2                 currency_buyer     0.000000\n",
              "27                         theme     0.000000\n",
              "28                   crawl_month     0.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c49c14fe-789f-4e64-beef-02bdb2fd36b7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Feature_names</th>\n",
              "      <th>Importances</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>rating_count</td>\n",
              "      <td>0.096420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>merchant_rating</td>\n",
              "      <td>0.057711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>price</td>\n",
              "      <td>0.053961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>badge_product_quality</td>\n",
              "      <td>0.036381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>merchant_name</td>\n",
              "      <td>0.033130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>countries_shipped_to</td>\n",
              "      <td>0.030667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>merchant_title</td>\n",
              "      <td>0.024724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>retail_price</td>\n",
              "      <td>0.021876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>merchant_rating_count</td>\n",
              "      <td>0.021862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>product_color</td>\n",
              "      <td>0.019974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>merchant_info_subtitle</td>\n",
              "      <td>0.014825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>urgency_text</td>\n",
              "      <td>0.008114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>units_sold</td>\n",
              "      <td>0.007463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>uses_ad_boosts</td>\n",
              "      <td>0.005845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>origin_country</td>\n",
              "      <td>0.004706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>badge_local_product</td>\n",
              "      <td>0.004437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>shipping_option_name</td>\n",
              "      <td>0.004234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>has_urgency_banner</td>\n",
              "      <td>0.003070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>shipping_option_price</td>\n",
              "      <td>0.001316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>product_variation_inventory</td>\n",
              "      <td>0.001316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>product_variation_size_id</td>\n",
              "      <td>0.001316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>merchant_has_profile_picture</td>\n",
              "      <td>0.000273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>inventory_total</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>shipping_is_express</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>badge_fast_shipping</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>badges_count</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>currency_buyer</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>theme</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>crawl_month</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c49c14fe-789f-4e64-beef-02bdb2fd36b7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c49c14fe-789f-4e64-beef-02bdb2fd36b7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c49c14fe-789f-4e64-beef-02bdb2fd36b7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#drop the values that low importance\n",
        "drop_values=df.loc[df['Importances']<0.001]"
      ],
      "metadata": {
        "id": "LaAL58S02GeV"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make it into a list\n",
        "drop=drop_values['Feature_names']\n",
        "d=drop.tolist()"
      ],
      "metadata": {
        "id": "CWF0e0qr3Lml"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "0RY24uOV30uQ"
      },
      "outputs": [],
      "source": [
        "tr_y_1 = tr['rating']\n",
        "#training data target values\n",
        "tr_x_1 = tr.drop('rating', axis=1)\n",
        "#drop non-important features\n",
        "tr_x_1=tr_x_1.drop(d,axis=1)\n",
        "#training data\n",
        "# Create Decision Tree classifer object\n",
        "clf_1 = DecisionTreeClassifier(criterion='entropy',max_depth=6)\n",
        "\n",
        "# Train Decision Tree Classifer\n",
        "clf_1 = clf_1.fit(tr_x_1,tr_y_1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "C6yPmLvz4LDD"
      },
      "outputs": [],
      "source": [
        "#validation x feature dropping\n",
        "val_x_1 = val.drop('rating', axis=1)\n",
        "#drop non important features\n",
        "val_x_1=val_x.drop(d,axis=1)\n",
        "#prediction\n",
        "pred_val_1 = clf_1.predict(val_x_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "hmVo5B_g4VjR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31824211-360a-4829-b257-d26312fcd607"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6726726726726727\n"
          ]
        }
      ],
      "source": [
        "#generate validation score\n",
        "val_score = f1_score(val_y, pred_val_1, average='micro')\n",
        "print(val_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "LpGr8N0i0GnJ"
      },
      "outputs": [],
      "source": [
        "# once you are happy with your local model, let's prepare a submission\n",
        "# we need to apply the same preprocessing steps on the testing set as you did before you train the model\n",
        "test_data_1 = test_data.drop(d, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attempt3: Prepruning by finding the most suitable maximum depth and maximum feature\n",
        "(inherit the reducted features in attempt2)"
      ],
      "metadata": {
        "id": "127AnRujIR2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use the gridsearchCV module to find the best parameter set\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "#try different parameters\n",
        "parameters={'max_depth':[2,4,6,8,10,12,14,16,18,20],'max_features':[2,4,6,8,10,12,14,16,18,20,22,24,25,26]}\n",
        "#construct decsiontree\n",
        "default_clf=DecisionTreeClassifier()\n",
        "#try tuning the decision tree using grid search\n",
        "grid_search=GridSearchCV(estimator=default_clf,param_grid=parameters)\n",
        "grid_search.fit(tr_x_1,tr_y_1)\n",
        "#DecisionTreeClassifier().get_params().keys()"
      ],
      "metadata": {
        "id": "RgA2ibo-IdNN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75d4e94c-730b-4589-db80-24e9b45c7404"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "200 fits failed out of a total of 700.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "200 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\", line 942, in fit\n",
            "    X_idx_sorted=X_idx_sorted,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\", line 308, in fit\n",
            "    raise ValueError(\"max_features must be in (0, n_features]\")\n",
            "ValueError: max_features must be in (0, n_features]\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.71447368 0.73421053 0.74473684 0.73947368 0.75131579 0.76184211\n",
            " 0.75921053 0.76315789 0.76578947 0.77236842        nan        nan\n",
            "        nan        nan 0.72763158 0.73289474 0.75       0.75526316\n",
            " 0.76184211 0.77631579 0.76973684 0.76842105 0.775      0.78157895\n",
            "        nan        nan        nan        nan 0.71184211 0.74210526\n",
            " 0.74210526 0.76447368 0.76184211 0.76315789 0.75789474 0.76842105\n",
            " 0.75921053 0.76578947        nan        nan        nan        nan\n",
            " 0.72368421 0.71447368 0.74868421 0.75657895 0.74868421 0.76842105\n",
            " 0.74868421 0.74605263 0.76578947 0.75921053        nan        nan\n",
            "        nan        nan 0.72105263 0.73947368 0.74210526 0.72894737\n",
            " 0.72631579 0.75131579 0.73684211 0.72894737 0.73552632 0.76052632\n",
            "        nan        nan        nan        nan 0.69868421 0.71184211\n",
            " 0.7        0.72236842 0.73815789 0.74605263 0.72368421 0.73289474\n",
            " 0.71052632 0.73157895        nan        nan        nan        nan\n",
            " 0.68421053 0.70657895 0.70263158 0.71052632 0.71710526 0.73421053\n",
            " 0.69473684 0.72105263 0.70263158 0.72105263        nan        nan\n",
            "        nan        nan 0.67763158 0.68421053 0.66842105 0.69210526\n",
            " 0.68947368 0.71578947 0.71842105 0.70131579 0.70263158 0.7\n",
            "        nan        nan        nan        nan 0.64210526 0.70526316\n",
            " 0.69473684 0.70657895 0.69605263 0.72236842 0.69210526 0.69210526\n",
            " 0.70657895 0.70657895        nan        nan        nan        nan\n",
            " 0.66052632 0.69210526 0.69868421 0.68684211 0.71842105 0.68552632\n",
            " 0.70394737 0.71315789 0.71184211 0.69605263        nan        nan\n",
            "        nan        nan]\n",
            "  category=UserWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(estimator=DecisionTreeClassifier(),\n",
              "             param_grid={'max_depth': [2, 4, 6, 8, 10, 12, 14, 16, 18, 20],\n",
              "                         'max_features': [2, 4, 6, 8, 10, 12, 14, 16, 18, 20,\n",
              "                                          22, 24, 25, 26]})"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#find the best parameter set \n",
        "model=grid_search.best_estimator_\n",
        "#training\n",
        "model.fit(tr_x_1,tr_y_1)\n",
        "#it turns out the best maximum depth is =2, features=18"
      ],
      "metadata": {
        "id": "xFTn0ejKMvCz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6156dba5-a7b2-4a58-a350-b43edbbbf18d"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(max_depth=4, max_features=20)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "x-9WB3xDNMYT"
      },
      "outputs": [],
      "source": [
        "#prediction\n",
        "pred_val_2 = model.predict(val_x_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "uW-94eA5NU3N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7470eae9-f70c-4259-ce83-56463cd893a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7477477477477479\n"
          ]
        }
      ],
      "source": [
        "#find f1_score\n",
        "val_score = f1_score(val_y, pred_val_2, average='micro')\n",
        "print(val_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "cr8dm-9T0GnJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "pred_test = model.predict(test_data_1)#predict from test data\n",
        "pred_df = pd.DataFrame(data={'id': np.asarray(_id), 'rating': pred_test})\n",
        "#generate csv file\n",
        "pred_df.to_csv('pred_walkthrough.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM"
      ],
      "metadata": {
        "id": "vCdFDLh64kDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#feature scaling for svm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# Initialise the Scaler\n",
        "scaler = StandardScaler()\n",
        " \n",
        "tr_svm=tr.copy()\n",
        "val_svm=val.copy()\n",
        "#normalization\n",
        "scaler.fit(tr_svm)\n",
        "scaler.fit(val_svm)\n",
        "#x-y split\n",
        "tr_y_svm = tr_svm['rating']\n",
        "tr_x_svm = tr_svm.drop('rating', axis=1)\n",
        "val_y_svm = val_svm['rating']\n",
        "val_x_svm = val_svm.drop('rating', axis=1)"
      ],
      "metadata": {
        "id": "1DCUTRitXdEQ"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attempt1: default configuration"
      ],
      "metadata": {
        "id": "beH0PmEhOt9T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "MMTgj2LT0GnK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0229d90a-2193-4340-b99b-567f604dfb0f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC()"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "#Import svm model\n",
        "from sklearn import svm\n",
        "\n",
        "#Default svm rbf classifier\n",
        "SVM_clf = svm.SVC() \n",
        "\n",
        "#Model training\n",
        "SVM_clf.fit(tr_x_svm, tr_y_svm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "TNTPKAk5UWHT"
      },
      "outputs": [],
      "source": [
        "#prediction\n",
        "pred_val_svm_1 = SVM_clf.predict(val_x_svm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "851d9d20-7cf5-4f73-a58b-c114432c613b",
        "id": "ceO7B7hIWt9s"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7027027027027027\n"
          ]
        }
      ],
      "source": [
        "val_score = f1_score(val_y_svm, pred_val_svm_1, average='micro')\n",
        "print(val_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attempt2: \n",
        "For the SVM model in this case, a linear model would be bad since there are too much features in training data. Thus, feature selection is not good for this case too since I decided to use a non linear SVM model. For similar reason, polynomial kernel is also bad.\n",
        "\n"
      ],
      "metadata": {
        "id": "_QDkuzfdW2zw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Therefore, in this attempt, I will try sigmoid kenrel with gamma=0.002, and see if tuning the kernel and gamma has any effect on the result."
      ],
      "metadata": {
        "id": "7CA18bXKf70r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92f95c14-c5e9-497d-a4e5-48b7c535cd19",
        "id": "N8yXQ9Y6gLc1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(gamma=0.002, kernel='sigmoid')"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "#Default svm linear classifier\n",
        "SVM_clf_2 = svm.SVC(kernel='sigmoid',gamma=0.002) \n",
        "\n",
        "\n",
        "#Model training\n",
        "SVM_clf_2.fit(tr_x_svm, tr_y_svm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "ZTXa9ddmg_cr"
      },
      "outputs": [],
      "source": [
        "#prediction\n",
        "pred_val_svm_2 = SVM_clf_2.predict(val_x_svm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3f95306-ee49-4051-dcf3-f5e7f10fad5f",
        "id": "tpxi_5vehBqS"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7027027027027027\n"
          ]
        }
      ],
      "source": [
        "#find f1_score\n",
        "val_score = f1_score(val_y_svm, pred_val_svm_2, average='micro')\n",
        "print(val_score)\n",
        "#no significant change from before"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attempt3:  This time, try the rbf kernel with balanced classweight and a larger gamma. It could improve the f1 score since there is a way to adjust the class weight now."
      ],
      "metadata": {
        "id": "-k36qbghhaHD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4f35f89-72b6-4bcb-c49c-a292cadcc91b",
        "id": "Z6948_wXizE1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(class_weight='balanced', gamma=0.1)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "#Default svm linear classifier\n",
        "SVM_clf_3 = svm.SVC(gamma=0.1,class_weight='balanced') \n",
        "\n",
        "\n",
        "#Model training\n",
        "SVM_clf_3.fit(tr_x_svm, tr_y_svm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "23oK2-SfjFKb"
      },
      "outputs": [],
      "source": [
        "#predictions\n",
        "pred_val_svm_3 = SVM_clf_3.predict(val_x_svm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd21ec88-38b8-4601-b233-babee4d879b1",
        "id": "Hek8vhTHjGMC"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7447447447447447\n"
          ]
        }
      ],
      "source": [
        "val_score = f1_score(val_y_svm, pred_val_svm_3, average='micro')\n",
        "print(val_score)\n",
        "#shows improvement in validation score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "396c4d46-a536-4ff4-d453-903a52f1becd",
        "id": "UInkHWdgj9Bg"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7417417417417418\n"
          ]
        }
      ],
      "source": [
        "#Default svm linear classifier\n",
        "SVM_clf_3_1 = svm.SVC(gamma=0.31,class_weight='balanced') \n",
        "#try to play with different gamma settings, \n",
        "#there starts to be reductions in f1_score  starts at gamma >0.3\n",
        "\n",
        "\n",
        "#Model training\n",
        "SVM_clf_3_1.fit(tr_x_svm, tr_y_svm)\n",
        "#Predictions\n",
        "pred_val_svm_3_1 = SVM_clf_3_1.predict(val_x_svm)\n",
        "val_score = f1_score(val_y_svm, pred_val_svm_3_1, average='micro')\n",
        "print(val_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Naive-Bayes\n",
        "Attempt1: default"
      ],
      "metadata": {
        "id": "1_WTikEZoxJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import Gaussian Naive Bayes model\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "#Create a Gaussian Classifier\n",
        "gnb = GaussianNB()\n",
        "\n",
        "#Model training using original data\n",
        "gnb.fit(tr_x, tr_y)\n",
        "\n",
        "#Prediction\n",
        "pred_val_naivebayes = gnb.predict(val_x)\n",
        "val_score = f1_score(val_y, pred_val_naivebayes, average='micro')\n",
        "print(val_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E59LKaAUkvtc",
        "outputId": "33cf0d68-f960-4dae-a9ad-ad6453d4b751"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5165165165165165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attemp2: Feature selection"
      ],
      "metadata": {
        "id": "SYbWRj38qdxW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Naive-bayes model, hyper paramter tuning is not gonna improve the metric greatly. A feature selection may be much more plausible. In this case, the correlated features will be removed.\n",
        "I would expect the scores to improve."
      ],
      "metadata": {
        "id": "RjkgDfxDqhol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correlated_features =set()#empty set to put the features in \n",
        "correlation_matrix = tr_x.corr()#generate the correlation matrix"
      ],
      "metadata": {
        "id": "tR1B-9i9qwMN"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_range=len(correlation_matrix .columns)\n",
        "#get the correlated features\n",
        "for x in range(column_range):#column\n",
        "    for y in range(x):#row\n",
        "        corr_=abs(correlation_matrix.iloc[x, y])#correlation socre\n",
        "        if abs(correlation_matrix.iloc[x, y]) > 0.8:#the feature is correlated if the correlation score>0.8\n",
        "            feature = correlation_matrix.columns[x]#record the feature\n",
        "            correlated_features.add(feature)"
      ],
      "metadata": {
        "id": "8uUQc97MsaQc"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#correlated features as shown\n",
        "print(correlated_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTCxT8OXsbw6",
        "outputId": "1780ffb1-8208-4b39-e8ea-4935acc55c91"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'badge_product_quality', 'urgency_text', 'rating_count', 'shipping_option_price'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#drop the correlated features\n",
        "tr_x_bayes=tr_x.drop(correlated_features,axis=1)"
      ],
      "metadata": {
        "id": "HbDLEJWGtDv9"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Naive bayes model\n",
        "gnb = GaussianNB()\n",
        "\n",
        "#Model Training\n",
        "gnb.fit(tr_x_bayes, tr_y)\n",
        "\n",
        "#Prediction\n",
        "val_x_bayes=val_x.drop(correlated_features,axis=1)\n",
        "pred_val_naivebayes_1 = gnb.predict(val_x_bayes)\n",
        "val_score = f1_score(val_y, pred_val_naivebayes_1, average='micro')\n",
        "print(val_score)\n",
        "\n",
        "#show improvements in validation scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLfNPwM1tf7f",
        "outputId": "73327360-21a5-490f-fd72-7394fb11cd78"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5465465465465466\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final prediction submission used the result from Attempt3 in the Decision Tree model section."
      ],
      "metadata": {
        "id": "rpXutwJKQule"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questionsï¼š<br>\n",
        "ðŸŒˆQ:Why Data Mining is a misnomer? What is another preferred name?<br>\n",
        "A: Because the goal of 'Data mining' is to analyze the patterns and extract knowledge of existing data, instead of extracting the data itself. A more prefered name would be Knowledge Discovery in Data (KDD).<br>\n",
        "ðŸŒˆ What is the general knowledge discovery process? What is the difference between a data engineer and data scientist/AI engineer?<br>\n",
        "A: The general knowledge discovery process is defined as he nontrivial process of identifying valid, novel, potentially useful, and ultimately understandable patterns in data. In the Datascience hierachy of needs, Data engineers handles the movements/storation of data and Data scientists explore,aggregate/label and optimize the data. A more specific description would be, Data engineers collects the data and transform the data into \"pipelines\" for the Data scientists to analyze,test, aggregate and optimize.<br>\n",
        "ðŸŒˆ In data mining, what is the difference between prediction and categorization?<br>\n",
        "A:Prediction is using exisiting data to identify a missing or unavailable value.Categorization doesn't generate new numercial values but uses the similarities between exsiting data points to determine which class they belong to(identify their labels).<br>\n",
        "ðŸŒˆ Why data science/machine learning is a bad idea in the context of information security?<br>\n",
        "A: Data science/machine learning relys on using large amounts of existing data for model training. It leaves spaces for information leak and recovering original private data through reverse engineering. <br>\n",
        "ðŸŒˆ What is CIA principle and how can we use it to access the security/privacy aspect of the AI system/pipelines?<br>\n",
        "A: The CIA principal is C(confientiality),I(integrity) and A(Availability).Confidentiality is another name for privacy. Integrity stands for maintaining the consistency, accuracy and trustworthiness of data over its entire lifecycle.Availability stands for that information should be consistently and readily accessible for authorized parties. The privacy/security aspect of the AI system could be examined using this principal with the following questions:<br>\n",
        "&emsp;Confidentiality: Under what circumstance, the disclosure of personal or confidential information would happen to the system.<br>\n",
        "&emsp;Integrity: Under what circumstance, the data would be corrupted or the AI system will be executed wrongly?<br>\n",
        "&emsp;Availability:Under what circumstance, the AI system will not provide any service unintended?<br>"
      ],
      "metadata": {
        "id": "PclUbgMTIL3_"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}